import json
import py_vncorenlp
from mint.text_graph import TextGraph
import os
from datetime import datetime

# IMPROVED ENTITY MATCHING FUNCTIONS
def segment_entity_with_vncorenlp(entity, model):
    """
    Segment entity sá»­ dá»¥ng VnCoreNLP Ä‘á»ƒ match vá»›i segmented text
    """
    try:
        result = model.annotate_text(entity)
        segmented_words = []
        if result and len(result) > 0:
            first_sentence = list(result.values())[0]
            for token in first_sentence:
                segmented_words.append(token["wordForm"])
        
        segmented_entity = "_".join(segmented_words)
        return segmented_entity
        
    except Exception as e:
        # Fallback: simple space to underscore replacement
        return entity.replace(" ", "_")

def improved_entity_matching(entity, sentence_text, model=None):
    """
    Improved entity matching vá»›i support cho segmented text
    """
    entity_lower = entity.lower()
    sentence_lower = sentence_text.lower()
    
    # Method 1: Direct matching
    if entity_lower in sentence_lower:
        return True
    
    # Method 2: Simple space->underscore replacement
    entity_simple_seg = entity.replace(" ", "_").lower()
    if entity_simple_seg in sentence_lower:
        return True
    
    # Method 3: VnCoreNLP segmentation
    if model:
        try:
            entity_vncorenlp_seg = segment_entity_with_vncorenlp(entity, model).lower()
            if entity_vncorenlp_seg in sentence_lower:
                return True
        except:
            pass
    
    # Method 4: Fuzzy matching cho partial matches
    entity_words = entity.split()
    if len(entity_words) > 1:
        all_words_found = True
        for word in entity_words:
            word_variants = [
                word.lower(),
                word.replace(" ", "_").lower()
            ]
            
            word_found = any(variant in sentence_lower for variant in word_variants)
            if not word_found:
                all_words_found = False
                break
        
        if all_words_found:
            return True
    
    return False

def improved_add_entities_to_graph(text_graph, entities, context_sentences, model):
    """
    Improved version cá»§a add_entities_to_graph vá»›i better matching
    """
    entity_nodes_added = []
    
    for entity in entities:
        # ThÃªm entity node
        entity_node = text_graph.add_entity_node(entity)
        entity_nodes_added.append(entity_node)
        
        connections_made = 0
        # TÃ¬m cÃ¡c sentences cÃ³ chá»©a entity nÃ y
        for sent_idx, sentence_node in text_graph.sentence_nodes.items():
            sentence_text = text_graph.graph.nodes[sentence_node]['text']
            
            # Sá»¬ Dá»¤NG IMPROVED MATCHING
            if improved_entity_matching(entity, sentence_text, model):
                text_graph.connect_entity_to_sentence(entity_node, sentence_node)
                connections_made += 1
                print(f"âœ… Improved: Káº¿t ná»‘i entity '{entity}' vá»›i sentence {sent_idx}")
        
        if connections_made == 0:
            print(f"âš ï¸ Entity '{entity}' khÃ´ng match vá»›i sentence nÃ o")
    
    print(f"ÄÃ£ thÃªm {len(entity_nodes_added)} entity nodes vÃ o graph vá»›i improved matching.")
    return entity_nodes_added

def process_sample_with_beam_search(sample_data, model, output_dir="beam_output"):
    """
    Xá»­ lÃ½ má»™t sample: xÃ¢y dá»±ng TextGraph vÃ  cháº¡y Beam Search vá»›i improved entity matching
    """
    context = sample_data["context"]
    claim = sample_data["claim"]
    evidence = sample_data["evidence"]
    label = sample_data["label"]
    
    print(f"Processing claim: {claim[:100]}...")
    
    try:
        # Xá»­ lÃ½ context vÃ  claim vá»›i VnCoreNLP
        context_sentences = model.annotate_text(context)
        claim_sentences = model.annotate_text(claim)
        
        # Táº¡o TextGraph
        text_graph = TextGraph()
        
        # Build basic graph tá»« VnCoreNLP output
        text_graph.build_from_vncorenlp_output(
            context_sentences, claim, claim_sentences
        )
        
        # ðŸ”§ IMPROVED: Extract vÃ  add entities vá»›i better matching
        print("ðŸ¤– Extracting entities with improved matching...")
        try:
            entities = text_graph.extract_entities_with_openai(context)
            if entities:
                entity_nodes = improved_add_entities_to_graph(
                    text_graph, entities, context_sentences, model
                )
                print(f"âœ… Added {len(entity_nodes)} entity nodes with improved matching")
        except Exception as e:
            print(f"âš ï¸ Entity extraction failed: {e}")
        
        # Cháº¡y Beam Search Ä‘á»ƒ tÃ¬m paths tá»« claim Ä‘áº¿n sentences
        paths = text_graph.beam_search_paths(
            beam_width=10,
            max_depth=6,
            max_paths=20
        )
        
        # TrÃ­ch xuáº¥t sentences tá»« paths
        beam_sentences = extract_sentences_from_paths(paths, text_graph)
        
        # Xá»­ lÃ½ format sentences (replace _ thÃ nh space)
        processed_sentences = []
        for sentence in beam_sentences:
            processed_sentence = sentence.replace("_", " ")
            processed_sentences.append(processed_sentence)
        
        # Táº¡o result
        result = {
            "context": context,
            "claim": claim,
            "evidence": evidence,
            "beam_evidence": processed_sentences,
            "label": label
        }
        
        return result, True
        
    except Exception as e:
        print(f"âŒ Error processing sample: {e}")
        return None, False

def extract_sentences_from_paths(paths, text_graph):
    """
    TrÃ­ch xuáº¥t danh sÃ¡ch sentences tá»« beam search paths
    """
    sentence_frequency = {}
    
    # Kiá»ƒm tra paths structure
    if not paths:
        print("âš ï¸  No paths found")
        return []
    
    # Äáº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a má»—i sentence
    for path_obj in paths:
        visited_sentences = set()
        
        # Kiá»ƒm tra cáº¥u trÃºc path object
        if hasattr(path_obj, 'nodes'):
            path_nodes = path_obj.nodes
        elif hasattr(path_obj, 'path'):
            path_nodes = path_obj.path
        elif isinstance(path_obj, dict) and 'nodes' in path_obj:
            path_nodes = path_obj['nodes']
        elif isinstance(path_obj, dict) and 'path' in path_obj:
            path_nodes = path_obj['path']
        elif hasattr(path_obj, '__dict__'):
            path_dict = path_obj.__dict__
            if 'nodes' in path_dict:
                path_nodes = path_dict['nodes']
            elif 'path' in path_dict:
                path_nodes = path_dict['path']
            else:
                continue
        else:
            continue
            
        for node_id in path_nodes:
            if node_id in text_graph.graph.nodes:
                node_data = text_graph.graph.nodes[node_id]
                if node_data.get('type') == 'sentence':
                    sentence_text = node_data.get('text', '')
                    if sentence_text and sentence_text not in visited_sentences:
                        sentence_frequency[sentence_text] = sentence_frequency.get(sentence_text, 0) + 1
                        visited_sentences.add(sentence_text)
    
    # Sáº¯p xáº¿p theo táº§n suáº¥t giáº£m dáº§n
    sorted_sentences = sorted(sentence_frequency.items(), key=lambda x: x[1], reverse=True)
    
    print(f"ðŸ“Š Found {len(sentence_frequency)} unique sentences")
    
    # Tráº£ vá» top sentences (giá»›i háº¡n 10 sentences)
    return [sentence for sentence, freq in sorted_sentences[:10]]

def main():
    """
    Main function Ä‘á»ƒ xá»­ lÃ½ toÃ n bá»™ dataset vá»›i improved entity matching
    """
    print("ðŸš€ Starting Beam Search processing with IMPROVED ENTITY MATCHING...")
    
    # LÆ°u working directory hiá»‡n táº¡i
    original_cwd = os.getcwd()
    print(f"ðŸ“‚ Original working directory: {original_cwd}")
    
    # Khá»Ÿi táº¡o VnCoreNLP model vá»›i Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    print("ðŸ“– Loading VnCoreNLP model...")
    vncorenlp_path = os.path.abspath("vncorenlp")
    model = py_vncorenlp.VnCoreNLP(save_dir=vncorenlp_path)
    
    # KhÃ´i phá»¥c working directory
    os.chdir(original_cwd)
    print(f"ðŸ“‚ Restored working directory: {os.getcwd()}")
    
    # Äá»c input file
    input_file = "raw_test.json"
    input_file_path = os.path.abspath(input_file)
    if not os.path.exists(input_file_path):
        print(f"âŒ File {input_file_path} not found!")
        return
    
    print(f"ðŸ“ Input file: {input_file_path}")
    
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ðŸ“Š Total samples: {len(data)}")
    
    # Táº¡o output directory
    os.makedirs("beam_output", exist_ok=True)
    
    # Xá»­ lÃ½ samples (test vá»›i 5 samples Ä‘áº§u tiÃªn)
    results = []
    success_count = 0
    total_beam_sentences = 0
    
    for i, sample in enumerate(data[:5]):  # Test vá»›i 5 samples
        print(f"\nðŸ“ Processing sample {i+1}/5...")
        
        result, success = process_sample_with_beam_search(sample, model)
        
        if success and result:
            results.append(result)
            success_count += 1
            
            beam_count = len(result.get("beam_evidence", []))
            total_beam_sentences += beam_count
            
            print(f"âœ… Sample {i+1}: Found {beam_count} sentences")
            # Show first 3 sentences
            for j, sentence in enumerate(result["beam_evidence"][:3], 1):
                print(f"   {j}. {sentence[:100]}...")
        else:
            print(f"âŒ Sample {i+1}: Failed to process")
    
    # LÆ°u káº¿t quáº£
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"beam_output/processed_with_improved_entities_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    # In thá»‘ng kÃª
    print(f"\nðŸŽ‰ Processing completed!")
    print(f"ðŸ“ Results saved to: {output_file}")
    print(f"ðŸ“Š Success rate: {success_count}/{len(data[:5])} ({success_count/5*100:.1f}%)")
    print(f"ðŸ“ˆ Statistics:")
    print(f"   - Total beam sentences found: {total_beam_sentences}")
    print(f"   - Average sentences per sample: {total_beam_sentences/max(success_count,1):.1f}")
    
    if success_count > 0:
        print(f"\nðŸ“‹ Sample result:")
        sample_result = results[0]
        print(f"   Claim: {sample_result['claim'][:100]}...")
        print(f"   Beam evidence found: {len(sample_result['beam_evidence'])} sentences")
        for i, sentence in enumerate(sample_result["beam_evidence"][:2], 1):
            print(f"      {i}. {sentence[:100]}...")

if __name__ == "__main__":
    main() 