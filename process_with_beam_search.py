import json
import py_vncorenlp
from mint.text_graph import TextGraph
import os
from datetime import datetime

def process_sample_with_beam_search(sample_data, model, output_dir="beam_output"):
    """
    Xá»­ lÃ½ má»™t sample: xÃ¢y dá»±ng TextGraph vÃ  cháº¡y Beam Search
    """
    context = sample_data["context"]
    claim = sample_data["claim"]
    evidence = sample_data["evidence"]
    label = sample_data["label"]
    
    print(f"Processing claim: {claim[:100]}...")
    
    try:
        # Xá»­ lÃ½ context vÃ  claim vá»›i VnCoreNLP
        context_sentences = model.annotate_text(context)
        claim_sentences = model.annotate_text(claim)
        
        # Táº¡o TextGraph
        text_graph = TextGraph()
        
        # Build graph tá»« VnCoreNLP output
        text_graph.build_from_vncorenlp_output(
            context_sentences, claim, claim_sentences
        )
        
        # Cháº¡y Beam Search Ä‘á»ƒ tÃ¬m paths tá»« claim Ä‘áº¿n sentences
        paths = text_graph.beam_search_paths(
            beam_width=10,
            max_depth=6,
            max_paths=20
        )
        
        # TrÃ­ch xuáº¥t sentences tá»« paths
        beam_sentences = extract_sentences_from_paths(paths, text_graph)
        
        # Xá»­ lÃ½ format sentences (replace _ thÃ nh space)
        processed_sentences = []
        for sentence in beam_sentences:
            processed_sentence = sentence.replace("_", " ")
            processed_sentences.append(processed_sentence)
        
        # Táº¡o result
        result = {
            "context": context,
            "claim": claim,
            "evidence": evidence,
            "beam_evidence": processed_sentences,
            "label": label
        }
        
        return result, True
        
    except Exception as e:
        print(f"Error processing sample: {e}")
        # Tráº£ vá» káº¿t quáº£ rá»—ng náº¿u cÃ³ lá»—i
        result = {
            "context": context,
            "claim": claim,
            "evidence": evidence,
            "beam_evidence": [],
            "label": label
        }
        return result, False

def extract_sentences_from_paths(paths, text_graph):
    """
    TrÃ­ch xuáº¥t danh sÃ¡ch sentences tá»« beam search paths
    """
    sentence_frequency = {}
    
    # Kiá»ƒm tra paths structure
    if not paths:
        print("âš ï¸  No paths found")
        return []
    
    # Äáº¿m táº§n suáº¥t xuáº¥t hiá»‡n cá»§a má»—i sentence
    for path_obj in paths:
        visited_sentences = set()
        
        # Kiá»ƒm tra cáº¥u trÃºc path object
        if hasattr(path_obj, 'nodes'):
            path_nodes = path_obj.nodes
        elif hasattr(path_obj, 'path'):
            path_nodes = path_obj.path
        elif isinstance(path_obj, dict) and 'nodes' in path_obj:
            path_nodes = path_obj['nodes']
        elif isinstance(path_obj, dict) and 'path' in path_obj:
            path_nodes = path_obj['path']
        elif hasattr(path_obj, '__dict__'):
            # Náº¿u lÃ  object cÃ³ attributes
            path_dict = path_obj.__dict__
            if 'nodes' in path_dict:
                path_nodes = path_dict['nodes']
            elif 'path' in path_dict:
                path_nodes = path_dict['path']
            else:
                continue
        else:
            continue
            
        for node_id in path_nodes:
            if node_id in text_graph.graph.nodes:
                node_data = text_graph.graph.nodes[node_id]
                if node_data.get('type') == 'sentence':
                    sentence_text = node_data.get('text', '')
                    if sentence_text and sentence_text not in visited_sentences:
                        sentence_frequency[sentence_text] = sentence_frequency.get(sentence_text, 0) + 1
                        visited_sentences.add(sentence_text)
    
    # Sáº¯p xáº¿p theo táº§n suáº¥t giáº£m dáº§n
    sorted_sentences = sorted(sentence_frequency.items(), key=lambda x: x[1], reverse=True)
    
    print(f"ğŸ“Š Found {len(sentence_frequency)} unique sentences")
    
    # Tráº£ vá» top sentences (giá»›i háº¡n 10 sentences)
    return [sentence for sentence, freq in sorted_sentences[:10]]

def main():
    """
    Main function Ä‘á»ƒ xá»­ lÃ½ toÃ n bá»™ dataset
    """
    print("ğŸš€ Starting Beam Search processing...")
    
    # LÆ°u working directory hiá»‡n táº¡i
    original_cwd = os.getcwd()
    print(f"ğŸ“‚ Original working directory: {original_cwd}")
    
    # Khá»Ÿi táº¡o VnCoreNLP model vá»›i Ä‘Æ°á»ng dáº«n tuyá»‡t Ä‘á»‘i
    print("ğŸ“– Loading VnCoreNLP model...")
    vncorenlp_path = os.path.abspath("vncorenlp")
    model = py_vncorenlp.VnCoreNLP(save_dir=vncorenlp_path)
    
    # KhÃ´i phá»¥c working directory
    os.chdir(original_cwd)
    print(f"ğŸ“‚ Restored working directory: {os.getcwd()}")
    
    # Äá»c input file
    input_file = "raw_test.json"
    input_file_path = os.path.abspath(input_file)
    if not os.path.exists(input_file_path):
        print(f"âŒ File {input_file_path} not found!")
        return
    
    print(f"ğŸ“ Input file: {input_file_path}")
    
    with open(input_file_path, 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    print(f"ğŸ“Š Loaded {len(data)} samples from {input_file}")
    
    # Táº¡o thÆ° má»¥c output
    os.makedirs("beam_output", exist_ok=True)
    
    # Xá»­ lÃ½ samples (test vá»›i 5 samples Ä‘áº§u)
    test_samples = data[:5]  # Chá»‰ test 5 samples Ä‘áº§u
    results = []
    success_count = 0
    
    for i, sample in enumerate(test_samples, 1):
        print(f"\nğŸ“ Processing sample {i}/{len(test_samples)}...")
        
        result, success = process_sample_with_beam_search(sample, model)
        results.append(result)
        
        if success:
            success_count += 1
            print(f"âœ… Sample {i}: Found {len(result['beam_evidence'])} sentences")
            # Hiá»ƒn thá»‹ má»™t sá»‘ sentences tÃ¬m Ä‘Æ°á»£c
            for j, sentence in enumerate(result['beam_evidence'][:3], 1):
                print(f"   {j}. {sentence[:100]}...")
        else:
            print(f"âŒ Sample {i}: Failed to process")
    
    # LÆ°u káº¿t quáº£
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"beam_output/processed_with_beam_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(results, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ‰ Processing completed!")
    print(f"ğŸ“ Results saved to: {output_file}")
    print(f"ğŸ“Š Success rate: {success_count}/{len(test_samples)} ({success_count/len(test_samples)*100:.1f}%)")
    
    # Hiá»ƒn thá»‹ thá»‘ng kÃª tá»•ng quan
    total_beam_sentences = sum(len(r['beam_evidence']) for r in results)
    avg_beam_sentences = total_beam_sentences / len(results) if results else 0
    
    print(f"ğŸ“ˆ Statistics:")
    print(f"   - Total beam sentences found: {total_beam_sentences}")
    print(f"   - Average sentences per sample: {avg_beam_sentences:.1f}")
    
    # Hiá»ƒn thá»‹ má»™t sample káº¿t quáº£
    if results and results[0]['beam_evidence']:
        print(f"\nğŸ“‹ Sample result:")
        sample_result = results[0]
        print(f"   Claim: {sample_result['claim'][:100]}...")
        print(f"   Beam evidence found: {len(sample_result['beam_evidence'])} sentences")
        for i, sentence in enumerate(sample_result['beam_evidence'][:2], 1):
            print(f"      {i}. {sentence[:80]}...")

if __name__ == "__main__":
    main() 