# üéØ MINT TextGraph - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng Beam Search Analysis

H∆∞·ªõng d·∫´n chi ti·∫øt v·ªÅ vi·ªác s·ª≠ d·ª•ng c√°c c√¥ng c·ª• ph√¢n t√≠ch k·∫øt qu·∫£ Beam Search trong MINT TextGraph ƒë·ªÉ ƒë√°nh gi√° hi·ªáu qu·∫£ fact-checking.

## üìã M·ª•c l·ª•c

1. [T·ªïng quan ch·ª©c nƒÉng](#-t·ªïng-quan-ch·ª©c-nƒÉng)
2. [Chu·∫©n b·ªã tr∆∞·ªõc khi s·ª≠ d·ª•ng](#-chu·∫©n-b·ªã-tr∆∞·ªõc-khi-s·ª≠-d·ª•ng)
3. [H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng t·ª´ng c√¥ng c·ª•](#-h∆∞·ªõng-d·∫´n-s·ª≠-d·ª•ng-t·ª´ng-c√¥ng-c·ª•)
4. [Workflow khuy·∫øn ngh·ªã](#-workflow-khuy·∫øn-ngh·ªã)
5. [Hi·ªÉu k·∫øt qu·∫£ ph√¢n t√≠ch](#-hi·ªÉu-k·∫øt-qu·∫£-ph√¢n-t√≠ch)
6. [X·ª≠ l√Ω s·ª± c·ªë](#-x·ª≠-l√Ω-s·ª±-c·ªë)

## üîç T·ªïng quan ch·ª©c nƒÉng

### H·ªá th·ªëng Beam Search Analysis g·ªìm 2 c√¥ng c·ª• ch√≠nh:

#### üî¨ **1. analyze_beam_sentences.py** - Ph√¢n t√≠ch th·ªëng k√™ to√†n di·ªán
```
üìä Ch·ª©c nƒÉng ch√≠nh:
‚îú‚îÄ‚îÄ Ph√¢n t√≠ch performance t·ªïng th·ªÉ c·ªßa beam search
‚îú‚îÄ‚îÄ T·∫°o bi·ªÉu ƒë·ªì tr·ª±c quan v·ªÅ scores, lengths, success rates
‚îú‚îÄ‚îÄ Ph√¢n t√≠ch patterns trong ƒë∆∞·ªùng ƒëi (paths)
‚îú‚îÄ‚îÄ T·∫°o b√°o c√°o comprehensive v·ªõi insights
‚îú‚îÄ‚îÄ Xu·∫•t th·ªëng k√™ chi ti·∫øt d·∫°ng CSV
‚îú‚îÄ‚îÄ X·∫øp h·∫°ng c√¢u quan tr·ªçng nh·∫•t cho t·ª´ng sample
‚îî‚îÄ‚îÄ T√≥m t·∫Øt top sentences v·ªõi scoring advanced
```

**üí° Khi n√†o s·ª≠ d·ª•ng:**
- Mu·ªën ƒë√°nh gi√° hi·ªáu su·∫•t beam search tr√™n to√†n b·ªô dataset
- So s√°nh performance gi·ªØa c√°c c·∫•u h√¨nh kh√°c nhau
- T√¨m hi·ªÉu xu h∆∞·ªõng v√† patterns trong k·∫øt qu·∫£
- T·∫°o b√°o c√°o t·ªïng quan cho nghi√™n c·ª©u
- Xem c√¢u n√†o quan tr·ªçng nh·∫•t cho fact-checking
- C·∫ßn k·∫øt qu·∫£ c·ª• th·ªÉ ƒë·ªÉ ki·ªÉm tra manual

#### üóÇÔ∏è **2. process_beam_samples.py** - T·ªï ch·ª©c d·ªØ li·ªáu theo claim
```
üîß Ch·ª©c nƒÉng ch√≠nh:
‚îú‚îÄ‚îÄ Nh√≥m sentences theo claim t∆∞∆°ng ·ª©ng
‚îú‚îÄ‚îÄ Lo·∫°i b·ªè duplicate v√† s·∫Øp x·∫øp
‚îú‚îÄ‚îÄ T·∫°o c·∫•u tr√∫c d·ªØ li·ªáu claim -> sentences
‚îú‚îÄ‚îÄ Validation v√† backup t·ª± ƒë·ªông
‚îî‚îÄ‚îÄ T·∫°o b√°o c√°o processing detailed
```

**üí° Khi n√†o s·ª≠ d·ª•ng:**
- C·∫ßn d·ªØ li·ªáu c√≥ c·∫•u tr√∫c ƒë·ªÉ t√≠ch h·ª£p h·ªá th·ªëng
- Mu·ªën tra c·ª©u nhanh sentences cho m·ªôt claim
- Chu·∫©n b·ªã d·ªØ li·ªáu cho analysis tools kh√°c
- Export d·ªØ li·ªáu cho external applications

## üõ†Ô∏è Chu·∫©n b·ªã tr∆∞·ªõc khi s·ª≠ d·ª•ng

### B∆∞·ªõc 1: Ki·ªÉm tra m√¥i tr∆∞·ªùng
```bash
# Ki·ªÉm tra Python version (y√™u c·∫ßu >= 3.7)
python --version

# C√†i ƒë·∫∑t dependencies c·∫ßn thi·∫øt
pip install pandas matplotlib seaborn numpy
```

### B∆∞·ªõc 2: T·∫°o d·ªØ li·ªáu beam search
```bash
# Ch·∫°y beam search cho m·ªôt sample test
python main.py --demo --beam-search --beam-width 10 --beam-max-depth 6 --verbose

# Ch·∫°y beam search cho nhi·ªÅu samples (v√≠ d·ª• 0-99)
for i in {0..99}; do
    echo "üîÑ Processing sample $i..."
    python main.py --idx $i --beam-search --beam-width 10 --beam-max-depth 6 --disable-visualization --quiet
done
```

### B∆∞·ªõc 3: Ki·ªÉm tra d·ªØ li·ªáu
```bash
# Ki·ªÉm tra output directory
ls output/beam_search_*.json | wc -l  # ƒê·∫øm s·ªë file beam search

# Ki·ªÉm tra m·ªôt file m·∫´u
head -20 output/beam_search_0.json
```

## üìö H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng t·ª´ng c√¥ng c·ª•

### üî¨ **Tool 1: Ph√¢n t√≠ch th·ªëng k√™ to√†n di·ªán**

#### C√°ch ch·∫°y:
```bash
python analyze_beam_sentences.py
```

#### Qu√° tr√¨nh th·ª±c hi·ªán:
```
üîç Analyzing beam search results...
üìä Successfully loaded 100 beam search samples
üìä Analyzing individual samples...
   Processed 10/100 samples...
   Processed 20/100 samples...
   ...
üíæ Saved detailed statistics to output/beam_search_detailed_stats.csv
üìä Creating visualizations...
üìä Saved output/beam_search_scores_analysis.png
üìä Saved output/beam_search_lengths_analysis.png
üìä Saved output/beam_search_rates_analysis.png
üìä Saved output/beam_search_coverage_analysis.png
üìä Saved output/beam_search_diversity_analysis.png
üîç Analyzing path patterns...
üíæ Saved path patterns to output/beam_search_patterns.json
üìÑ Saved comprehensive report to output/beam_search_comprehensive_report.txt
‚úÖ Analysis completed successfully!
```

#### Files ƒë∆∞·ª£c t·∫°o ra:
```
output/
‚îú‚îÄ‚îÄ beam_search_detailed_stats.csv           # üìä D·ªØ li·ªáu th·ªëng k√™ chi ti·∫øt
‚îú‚îÄ‚îÄ beam_search_scores_analysis.png          # üìà Bi·ªÉu ƒë·ªì ƒëi·ªÉm s·ªë
‚îú‚îÄ‚îÄ beam_search_lengths_analysis.png         # üìè Bi·ªÉu ƒë·ªì ƒë·ªô d√†i path
‚îú‚îÄ‚îÄ beam_search_rates_analysis.png           # üìä Bi·ªÉu ƒë·ªì t·ª∑ l·ªá th√†nh c√¥ng
‚îú‚îÄ‚îÄ beam_search_coverage_analysis.png        # üéØ Bi·ªÉu ƒë·ªì ƒë·ªô ph·ªß t·ª´
‚îú‚îÄ‚îÄ beam_search_diversity_analysis.png       # üåê Bi·ªÉu ƒë·ªì ƒëa d·∫°ng nodes
‚îú‚îÄ‚îÄ beam_search_patterns.json               # üîç Patterns ph·ªï bi·∫øn
‚îú‚îÄ‚îÄ beam_search_comprehensive_report.txt    # üìã B√°o c√°o t·ªïng h·ª£p
‚îî‚îÄ‚îÄ beam_search_top_sentences.txt           # üìù Top sentences t·ª´ t·∫•t c·∫£ samples
```

#### V√≠ d·ª• k·∫øt qu·∫£ th·ªëng k√™:
```
üìà SUMMARY STATISTICS:
                avg_score  avg_length  sentence_reach_rate  entity_visit_rate
count           100.000000  100.000000          100.000000         100.000000
mean              8.234567    4.567890            0.756000           0.234000
std               2.123456    1.234567            0.198765           0.156789
min               3.456789    2.000000            0.200000           0.000000
max              14.567890    8.000000            1.000000           0.600000

üèÜ TOP 5 PERFORMING SAMPLES:
Sample 23: Score 14.568, Success Rate 100.0%
Sample 45: Score 13.234, Success Rate 95.0%
Sample 67: Score 12.890, Success Rate 90.0%
```

### üóÇÔ∏è **Tool 2: T·ªï ch·ª©c d·ªØ li·ªáu theo claim**

#### C√°ch ch·∫°y:
```bash
python process_beam_samples.py
```

#### Qu√° tr√¨nh th·ª±c hi·ªán:
```
üîÑ Starting Beam Search Samples Processing...
üìÅ Found 100 beam search files to process
   Processed 10/100 files...
   Processed 20/100 files...
   ...
‚úÖ Processing completed:
   Successfully processed: 100 files
   Errors encountered: 0 files
   Unique claims found: 98

üìä PROCESSING SUMMARY:
========================================
üìã Total unique claims: 98
üìù Claims with sentences: 95
üìÑ Total sentences across all claims: 2,156
üìä Average sentences per claim: 22.0
üìà Maximum sentences in a single claim: 45

üîç SAMPLE PREVIEW:
----------------------------------------
Sample 1:
Claim: Ph√≥ Th·ªß t∆∞·ªõng Tr·∫ßn H·ªìng H√† thay m·∫∑t Ch√≠nh ph·ªß, Th·ªß t∆∞·ªõng Ch√≠nh ph·ªß ch√∫c m·ª´ng...
Sentences (7):
  1. Thay_m·∫∑t Ch√≠nh_ph·ªß , Th·ªß_t∆∞·ªõng Ch√≠nh_ph·ªß , Ph√≥ Th·ªß_t∆∞·ªõng Tr·∫ßn_H·ªìng_H√† ch√∫c_m·ª´ng...
  2. Sau th·ªùi_gian gi√°n_ƒëo·∫°n do ƒë·∫°i_d·ªãch COVID-19 , Li√™n_hoan truy·ªÅn_h√¨nh to√†n_qu·ªëc...
  ... and 5 more sentences

üìÅ Created backup: beam_samples_organized_backup.json
‚úÖ Saved organized samples to beam_samples_organized.json
üìä Saved processing report to beam_samples_organized_report.txt
‚úÖ Processing completed successfully!
```

#### Files ƒë∆∞·ª£c t·∫°o ra:
```
‚îú‚îÄ‚îÄ beam_samples_organized.json             # üóÇÔ∏è D·ªØ li·ªáu t·ªï ch·ª©c ch√≠nh
‚îú‚îÄ‚îÄ beam_samples_organized_report.txt       # üìä B√°o c√°o validation  
‚îî‚îÄ‚îÄ beam_samples_organized_backup.json      # üíæ Backup t·ª± ƒë·ªông
```

#### C·∫•u tr√∫c d·ªØ li·ªáu output:
```json
{
  "Claim text ƒë·∫ßy ƒë·ªß ·ªü ƒë√¢y...": [
    "C√¢u 1 li√™n quan ƒë·∫øn claim n√†y...",
    "C√¢u 2 li√™n quan ƒë·∫øn claim n√†y...",
    "C√¢u 3 li√™n quan ƒë·∫øn claim n√†y..."
  ],
  "Claim kh√°c ·ªü ƒë√¢y...": [
    "C√¢u A li√™n quan ƒë·∫øn claim kh√°c...",
    "C√¢u B li√™n quan ƒë·∫øn claim kh√°c..."
  ]
}
```

## üîÑ Workflow khuy·∫øn ngh·ªã

### **Workflow c∆° b·∫£n (cho ng∆∞·ªùi m·ªõi b·∫Øt ƒë·∫ßu):**

```bash
# B∆∞·ªõc 1: Ch·∫°y beam search cho v√†i samples test
python main.py --idx 0 --beam-search --verbose
python main.py --idx 1 --beam-search --verbose
python main.py --idx 2 --beam-search --verbose

# B∆∞·ªõc 2: Ki·ªÉm tra k·∫øt qu·∫£ c√≥ h·ª£p l√Ω kh√¥ng
python analyze_beam_sentences.py

# B∆∞·ªõc 3: N·∫øu OK, ch·∫°y batch l·ªõn h∆°n
for i in {0..19}; do
    python main.py --idx $i --beam-search --disable-visualization --quiet
done

# B∆∞·ªõc 4: Ph√¢n t√≠ch to√†n di·ªán v√† t·ªï ch·ª©c d·ªØ li·ªáu
python analyze_beam_sentences.py
python process_beam_samples.py
```

### **Workflow n√¢ng cao (cho nghi√™n c·ª©u):**

```bash
# B∆∞·ªõc 1: Ch·∫°y beam search v·ªõi nhi·ªÅu c·∫•u h√¨nh
mkdir -p experiments/config1 experiments/config2

# Config 1: Beam width 10
for i in {0..99}; do
    python main.py --idx $i --beam-search --beam-width 10 --beam-max-depth 6 --disable-visualization --quiet
done
mv output/* experiments/config1/

# Config 2: Beam width 20  
for i in {0..99}; do
    python main.py --idx $i --beam-search --beam-width 20 --beam-max-depth 6 --disable-visualization --quiet
done
mv output/* experiments/config2/

# B∆∞·ªõc 2: So s√°nh k·∫øt qu·∫£
cd experiments/config1 && python ../../analyze_beam_sentences.py
cd ../config2 && python ../../analyze_beam_sentences.py

# B∆∞·ªõc 3: T·∫°o b√°o c√°o so s√°nh
python compare_experiments.py config1 config2
```

### **Workflow cho production:**

```bash
# B∆∞·ªõc 1: Chu·∫©n b·ªã environment
mkdir -p production_run/$(date +%Y%m%d_%H%M%S)
cd production_run/$(date +%Y%m%d_%H%M%S)

# B∆∞·ªõc 2: Ch·∫°y full dataset
python batch_process.py --samples 0-999 --beam-width 15 --beam-max-depth 8

# B∆∞·ªõc 3: Quality check
python analyze_beam_sentences.py
python validate_results.py

# B∆∞·ªõc 4: Export final results
python process_beam_samples.py
python export_for_annotation.py
```

## üìä Hi·ªÉu k·∫øt qu·∫£ ph√¢n t√≠ch

### **C√°c metrics quan tr·ªçng:**

#### 1. **Score (ƒêi·ªÉm s·ªë path)**
```
üéØ √ù nghƒ©a: ƒê·ªô li√™n quan c·ªßa path v·ªõi claim
üìä Gi√° tr·ªã t·ªët: > 8.0
‚ö†Ô∏è C·∫ßn ch√∫ √Ω: < 5.0
üí° C√°ch c·∫£i thi·ªán: TƒÉng similarity threshold, tune scoring weights
```

#### 2. **Sentence Reach Rate (T·ª∑ l·ªá ƒë·∫øn sentence)**
```
üéØ √ù nghƒ©a: % paths th√†nh c√¥ng t√¨m ƒë∆∞·ª£c evidence sentences
üìä Gi√° tr·ªã t·ªët: > 80%
‚ö†Ô∏è C·∫ßn ch√∫ √Ω: < 60%  
üí° C√°ch c·∫£i thi·ªán: TƒÉng beam width, max depth
```

#### 3. **Entity Visit Rate (T·ª∑ l·ªá ƒëi qua entity)**
```
üéØ √ù nghƒ©a: % paths ƒëi qua c√°c entity quan tr·ªçng
üìä Gi√° tr·ªã t·ªët: > 40%
‚ö†Ô∏è C·∫ßn ch√∫ √Ω: < 20%
üí° C√°ch c·∫£i thi·ªán: C·∫£i thi·ªán entity extraction, entity linking
```

#### 4. **Word Coverage (ƒê·ªô ph·ªß t·ª´)**
```
üéØ √ù nghƒ©a: % t·ª´ trong claim ƒë∆∞·ª£c match trong paths
üìä Gi√° tr·ªã t·ªët: > 70%
‚ö†Ô∏è C·∫ßn ch√∫ √Ω: < 50%
üí° C√°ch c·∫£i thi·ªán: C·∫£i thi·ªán semantic similarity, POS filtering
```

### **ƒê·ªçc hi·ªÉu patterns:**

#### Pattern ph·ªï bi·∫øn:
```
C->W->S (SUCCESS): Claim -> Word -> Sentence = T·ªët nh·∫•t
C->W->E->S (SUCCESS): Qua entity = R·∫•t t·ªët  
C->W->W->S (SUCCESS): Nhi·ªÅu word hops = OK
C->W->W (PARTIAL): Kh√¥ng ƒë·∫øn sentence = C·∫ßn c·∫£i thi·ªán
```

#### V√≠ d·ª• gi·∫£i th√≠ch:
```
Pattern: C->W->E->W->S (SUCCESS) +ENTITY
√ù nghƒ©a: 
- B·∫Øt ƒë·∫ßu t·ª´ claim (C)
- ƒêi qua word node (W) 
- ƒêi qua entity quan tr·ªçng (E)
- ƒêi qua word kh√°c (W)
- ƒê·∫øn ƒë∆∞·ª£c sentence evidence (S)
- ƒê√¢y l√† path l√Ω t∆∞·ªüng!
```

### **Ph√¢n t√≠ch c√¢u ƒë∆∞·ª£c rank:**

#### C√°ch ƒë·ªçc sentence ranking:
```
1. sentence_2 (visited 8x)
   Score: 20.1000
   Text: C√¢u evidence ch·∫•t l∆∞·ª£ng cao...

Gi·∫£i th√≠ch:
- sentence_2: ID c·ªßa c√¢u trong graph
- visited 8x: Xu·∫•t hi·ªán trong 8 paths kh√°c nhau
- Score 20.1: ƒêi·ªÉm t·ªïng h·ª£p (cao = t·ªët)
- Text: N·ªôi dung c√¢u ƒë·ªÉ human review
```

#### Ng∆∞·ª°ng ƒë√°nh gi√°:
```
üî• Score > 15: Evidence r·∫•t m·∫°nh, g·∫ßn nh∆∞ ch·∫Øc ch·∫Øn li√™n quan
‚úÖ Score 10-15: Evidence t·ªët, c√≥ kh·∫£ nƒÉng cao li√™n quan  
ü§î Score 5-10: Evidence trung b√¨nh, c·∫ßn ki·ªÉm tra manual
‚ö†Ô∏è Score < 5: Evidence y·∫øu, c√≥ th·ªÉ kh√¥ng li√™n quan
```

## üö® X·ª≠ l√Ω s·ª± c·ªë

### **L·ªói th∆∞·ªùng g·∫∑p v√† c√°ch kh·∫Øc ph·ª•c:**

#### 1. **Kh√¥ng t√¨m th·∫•y beam search files**
```bash
‚ö†Ô∏è No beam search results found. Make sure you have run beam search first.
```
**Nguy√™n nh√¢n:** Ch∆∞a ch·∫°y beam search ho·∫∑c files b·ªã x√≥a
**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra th∆∞ m·ª•c output
ls -la output/beam_search_*.json

# N·∫øu kh√¥ng c√≥ file, ch·∫°y l·∫°i beam search
python main.py --demo --beam-search --verbose

# Ki·ªÉm tra l·∫°i
ls -la output/beam_search_*.json
```

#### 2. **Files b·ªã corrupt ho·∫∑c kh√¥ng ƒë·ªçc ƒë∆∞·ª£c**
```bash
‚ùå Error loading beam_search_5.json: Invalid JSON format
```
**Nguy√™n nh√¢n:** File b·ªã h·ªèng do gi√°n ƒëo·∫°n process ho·∫∑c ƒë·∫ßy ƒëƒ©a
**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra dung l∆∞·ª£ng ƒëƒ©a
df -h

# Ki·ªÉm tra file b·ªã h·ªèng
python -m json.tool output/beam_search_5.json

# X√≥a file h·ªèng v√† ch·∫°y l·∫°i
rm output/beam_search_5.json
python main.py --idx 5 --beam-search --verbose
```

#### 3. **Thi·∫øu dependencies**
```bash
ModuleNotFoundError: No module named 'matplotlib'
```
**Gi·∫£i ph√°p:**
```bash
# C√†i ƒë·∫∑t ƒë·∫ßy ƒë·ªß dependencies
pip install -r requirements.txt

# Ho·∫∑c c√†i t·ª´ng package
pip install pandas matplotlib seaborn numpy

# Ki·ªÉm tra c√†i ƒë·∫∑t
python -c "import pandas, matplotlib, seaborn, numpy; print('All dependencies OK')"
```

#### 4. **Memory error v·ªõi dataset l·ªõn**
```bash
MemoryError: Unable to allocate array
```
**Gi·∫£i ph√°p:**
```bash
# Gi·∫£m batch size, x·ª≠ l√Ω t·ª´ng ph·∫ßn
python analyze_beam_sentences.py --batch-size 50

# Ho·∫∑c filter ch·ªâ files g·∫ßn ƒë√¢y
python analyze_beam_sentences.py --recent-only 100

# TƒÉng virtual memory (Linux)
sudo sysctl vm.overcommit_memory=1
```

#### 5. **Permission errors**
```bash
‚ùå Error saving file: Permission denied
```
**Gi·∫£i ph√°p:**
```bash
# Ki·ªÉm tra quy·ªÅn th∆∞ m·ª•c
ls -la output/

# T·∫°o l·∫°i th∆∞ m·ª•c v·ªõi quy·ªÅn ƒë√∫ng
rm -rf output/
mkdir output
chmod 755 output/

# Ho·∫∑c thay ƒë·ªïi owner
sudo chown -R $(whoami):$(whoami) output/
```

### **Debug v√† troubleshooting:**

#### Ki·ªÉm tra ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu:
```bash
# ƒê·∫øm s·ªë files
find output/ -name "beam_search_*.json" | wc -l

# Ki·ªÉm tra size files
du -sh output/beam_search_*.json | sort -h

# Ki·ªÉm tra files tr·ªëng
find output/ -name "beam_search_*.json" -size 0

# Validate JSON format
for file in output/beam_search_*.json; do
    echo "Checking $file..."
    python -m json.tool "$file" > /dev/null || echo "‚ùå $file is invalid"
done
```

#### Ch·∫°y trong debug mode:
```bash
# Ch·∫°y v·ªõi verbose output
python analyze_beam_sentences.py --verbose

# Ch·∫°y v·ªõi logging
python analyze_beam_sentences.py --log-level DEBUG

# Test v·ªõi sample nh·ªè
python analyze_beam_sentences.py --test-mode --samples 5
```

### **Performance tuning:**

#### Cho datasets l·ªõn:
```bash
# S·ª≠ d·ª•ng multiprocessing
python analyze_beam_sentences.py --workers 4

# Batch processing
python analyze_beam_sentences.py --batch-size 100

# Ch·ªâ t·∫°o summary, b·ªè qua visualization
python analyze_beam_sentences.py --no-plots
```

#### T·ªëi ∆∞u memory:
```bash
# X√≥a files t·∫°m
rm -rf output/temp_*

# Compress old files
gzip output/beam_search_*.json

# Cleanup sau khi ch·∫°y
python cleanup_analysis.py
```

## üí° Tips v√† best practices

### **T·ªëi ∆∞u hi·ªáu su·∫•t:**
1. **Ch·∫°y beam search theo batch nh·ªè** thay v√¨ m·ªôt l√∫c to√†n b·ªô
2. **Backup th∆∞·ªùng xuy√™n** th∆∞ m·ª•c output
3. **Monitor disk space** khi ch·∫°y datasets l·ªõn
4. **S·ª≠ d·ª•ng screen/tmux** cho long-running jobs

### **ƒê·∫£m b·∫£o ch·∫•t l∆∞·ª£ng:**
1. **Lu√¥n validate** k·∫øt qu·∫£ v·ªõi sample nh·ªè tr∆∞·ªõc
2. **So s√°nh k·∫øt qu·∫£** gi·ªØa c√°c l·∫ßn ch·∫°y
3. **Manual review** top sentences t·ª´ summarize tool
4. **Document parameters** ƒë√£ s·ª≠ d·ª•ng cho reproducibility

### **Integration v·ªõi tools kh√°c:**
```bash
# Export sang Excel
python export_to_excel.py output/beam_search_detailed_stats.csv

# T√≠ch h·ª£p v·ªõi Jupyter Notebook
jupyter notebook analysis_dashboard.ipynb

# API endpoint
python flask_api.py --port 5000
```

---

üéâ **Ch√∫c b·∫°n ph√¢n t√≠ch hi·ªáu qu·∫£!** N·∫øu c√≥ th·∫Øc m·∫Øc, h√£y ki·ªÉm tra log output v√† error messages ƒë·ªÉ debug. 