"""
Demo script Ä‘á»ƒ trá»±c quan hÃ³a dependency parsing trong TextGraph
"""

import py_vncorenlp
from os import path, getcwd
from mint import TextGraph

BASE_DIR = getcwd()
VNCORENLP_PATH = path.join(BASE_DIR, "vncorenlp")

# Khá»Ÿi táº¡o model má»™t láº§n duy nháº¥t
MODEL = None

def get_model():
    """Láº¥y model py_vncorenlp (singleton pattern)"""
    global MODEL
    if MODEL is None:
        MODEL = py_vncorenlp.VnCoreNLP(save_dir=VNCORENLP_PATH)
    return MODEL

def demo_simple_sentence():
    """Demo vá»›i cÃ¢u Ä‘Æ¡n giáº£n"""
    print("ğŸ” DEMO Vá»šI CÃ‚U ÄÃšN GIáº¢N")
    print("=" * 50)
    
    simple_context = "SAWACO thÃ´ng bÃ¡o táº¡m ngÆ°ng cung cáº¥p nÆ°á»›c."
    simple_claim = "SAWACO ngÆ°ng cung cáº¥p nÆ°á»›c."
    
    model = get_model()
    
    context_sentences = model.annotate_text(simple_context)
    claim_sentences = model.annotate_text(simple_claim)
    
    text_graph = TextGraph()
    text_graph.build_from_vncorenlp_output(context_sentences, simple_claim, claim_sentences)
    
    stats = text_graph.get_detailed_statistics()
    
    print(f"ğŸ“Š Thá»‘ng kÃª cÆ¡ báº£n:")
    print(f"  - Nodes: {stats['total_nodes']}")
    print(f"  - Edges: {stats['total_edges']} (Structural: {stats['structural_edges']}, Dependency: {stats['dependency_edges']})")
    print(f"  - Shared words: {stats['shared_words_count']}")
    
    print(f"\nğŸ”— Dependency relationships:")
    dep_stats = stats['dependency_statistics']
    for dep_type, count in dep_stats['most_common_dependencies']:
        print(f"  {dep_type}: {count}")
    
    print(f"\nğŸ“ Shared words giá»¯a context vÃ  claim:")
    for word_info in stats['shared_words']:
        print(f"  '{word_info['word']}' ({word_info['pos']})")
    
    # PhÃ¢n tÃ­ch dependency cá»§a tá»« SAWACO
    print(f"\nğŸ¯ Dependency analysis cá»§a 'SAWACO':")
    deps = text_graph.get_word_dependencies("SAWACO")
    if deps['heads']:
        print("  Heads:")
        for head in deps['heads']:
            print(f"    -> {head['word']} ({head['relation']})")
    if deps['dependents']:
        print("  Dependents:")
        for dep in deps['dependents']:
            print(f"    <- {dep['word']} ({dep['relation']})")
    
    # Uncomment Ä‘á»ƒ xem visualization
    # text_graph.visualize(show_dependencies=True)
    # text_graph.visualize_dependencies_only()
    
    return text_graph

def analyze_complex_dependencies():
    """PhÃ¢n tÃ­ch dependency relationships phá»©c táº¡p"""
    print("\nğŸ§  PHÃ‚N TÃCH DEPENDENCY PHá»¨C Táº P")
    print("=" * 50)
    
    complex_text = "CÃ´ng ty SAWACO Ä‘Ã£ thÃ´ng bÃ¡o viá»‡c táº¡m ngÆ°ng cung cáº¥p nÆ°á»›c sáº¡ch Ä‘á»ƒ thá»±c hiá»‡n báº£o trÃ¬ há»‡ thá»‘ng."
    
    model = get_model()
    sentences = model.annotate_text(complex_text)
    
    text_graph = TextGraph()
    # Táº¡o Ä‘á»“ thá»‹ chá»‰ vá»›i context
    text_graph.build_from_vncorenlp_output(sentences, "", {})
    
    dep_stats = text_graph.get_dependency_statistics()
    
    print(f"ğŸ“ˆ Dependency statistics:")
    print(f"  Total dependency edges: {dep_stats['total_dependency_edges']}")
    print(f"  Unique dependency types: {len(dep_stats['dependency_types'])}")
    
    print(f"\nğŸ·ï¸ Dependency types:")
    for dep_type, count in sorted(dep_stats['dependency_types'].items()):
        print(f"  {dep_type}: {count}")
    
    # PhÃ¢n tÃ­ch má»™t sá»‘ tá»« quan trá»ng
    important_words = ["SAWACO", "thÃ´ng_bÃ¡o", "cung_cáº¥p", "nÆ°á»›c"]
    
    print(f"\nğŸ” Dependency analysis cho tá»« khÃ³a:")
    for word in important_words:
        if word in text_graph.word_nodes:
            deps = text_graph.get_word_dependencies(word)
            print(f"\n  '{word}':")
            if deps['heads']:
                heads_str = [f"{h['word']}({h['relation']})" for h in deps['heads']]
                print(f"    Heads: {heads_str}")
            if deps['dependents']:
                deps_str = [f"{d['word']}({d['relation']})" for d in deps['dependents']]
                print(f"    Dependents: {deps_str}")
    
    return text_graph

def compare_structures():
    """So sÃ¡nh cáº¥u trÃºc dependency giá»¯a context vÃ  claim"""
    print("\nâš–ï¸ SO SÃNH Cáº¤U TRÃšC DEPENDENCY")
    print("=" * 50)
    
    context = "SAWACO thÃ´ng bÃ¡o táº¡m ngÆ°ng cung cáº¥p nÆ°á»›c tá»« 22 giá» ngÃ y 25-3."
    claim = "SAWACO ngÆ°ng cung cáº¥p nÆ°á»›c tá»« 12 giá» ngÃ y 25-3."
    
    model = get_model()
    
    context_sentences = model.annotate_text(context)
    claim_sentences = model.annotate_text(claim)
    
    text_graph = TextGraph()
    text_graph.build_from_vncorenlp_output(context_sentences, claim, claim_sentences)
    
    stats = text_graph.get_detailed_statistics()
    
    print(f"ğŸ“Š Graph overview:")
    print(f"  Shared words: {stats['shared_words_count']}/{stats['unique_words']}")
    print(f"  Dependency edges: {stats['dependency_edges']}")
    
    print(f"\nğŸ¯ Tá»« chung vÃ  dependency cá»§a chÃºng:")
    for word_info in stats['shared_words'][:5]:  # Top 5 shared words
        word = word_info['word']
        if len(word) > 1:  # Skip punctuation
            deps = text_graph.get_word_dependencies(word)
            print(f"  '{word}' ({word_info['pos']}):")
            if deps['heads']:
                print(f"    -> {[h['word'] for h in deps['heads']]}")
            if deps['dependents']:
                print(f"    <- {[d['word'] for d in deps['dependents']]}")

if __name__ == "__main__":
    print("ğŸš€ MINT TextGraph - Dependency Parsing Demo")
    print("=" * 60)
    
    # Demo 1: CÃ¢u Ä‘Æ¡n giáº£n
    graph1 = demo_simple_sentence()
    
    # Demo 2: Dependency phá»©c táº¡p  
    graph2 = analyze_complex_dependencies()
    
    # Demo 3: So sÃ¡nh cáº¥u trÃºc
    compare_structures()
    
    print(f"\nâœ… Demo hoÃ n thÃ nh!")
    print(f"ğŸ’¡ Äá»ƒ xem visualization, uncomment cÃ¡c dÃ²ng:")
    print(f"   - text_graph.visualize()")
    print(f"   - text_graph.visualize_dependencies_only()") 