#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Demo script cho tÃ­nh nÄƒng lá»c tá»« loáº¡i (POS filtering) trong MINT TextGraph
Minh há»a sá»± khÃ¡c biá»‡t giá»¯a Ä‘á»“ thá»‹ cÃ³ vÃ  khÃ´ng cÃ³ lá»c tá»« loáº¡i
"""

import json
from mint.text_graph import TextGraph
from mint.helpers import setup_vncorenlp, process_text_data

def compare_graphs():
    """So sÃ¡nh Ä‘á»“ thá»‹ vá»›i vÃ  khÃ´ng cÃ³ POS filtering"""
    
    # Load demo data
    with open('data/demo.json', 'r', encoding='utf-8') as f:
        data = json.load(f)
    
    context = data['context']
    claim = data['claim']
    
    print("ğŸ”¥ DEMO: So sÃ¡nh Ä‘á»“ thá»‹ cÃ³ vÃ  khÃ´ng cÃ³ POS filtering")
    print("="*60)
    
    # Setup VnCoreNLP
    print("ğŸ“¥ Äang setup VnCoreNLP...")
    model = setup_vncorenlp("vncorenlp", verbose=True)
    
    # Process text
    print("ğŸ” Äang xá»­ lÃ½ vÄƒn báº£n...")
    context_sentences, claim_sentences = process_text_data(model, context, claim, verbose=True)
    
    # Build graph WITHOUT POS filtering
    print("\nğŸ“Š XÃ¢y dá»±ng Ä‘á»“ thá»‹ KHÃ”NG cÃ³ lá»c tá»« loáº¡i...")
    graph_no_filter = TextGraph()
    graph_no_filter.set_pos_filtering(enable=False)
    graph_no_filter.build_from_vncorenlp_output(context_sentences, claim, claim_sentences)
    stats_no_filter = graph_no_filter.get_statistics()
    
    # Build graph WITH POS filtering  
    print("ğŸ“Š XÃ¢y dá»±ng Ä‘á»“ thá»‹ CÃ“ lá»c tá»« loáº¡i...")
    graph_with_filter = TextGraph()
    graph_with_filter.set_pos_filtering(enable=True)
    graph_with_filter.build_from_vncorenlp_output(context_sentences, claim, claim_sentences)
    stats_with_filter = graph_with_filter.get_statistics()
    
    # Compare results
    print("\n" + "="*60)
    print("ğŸ“ˆ Káº¾T QUáº¢ SO SÃNH")
    print("="*60)
    
    print("ğŸ”¸ KHÃ”NG lá»c tá»« loáº¡i:")
    print(f"  Total nodes: {stats_no_filter['total_nodes']}")
    print(f"  Word nodes: {stats_no_filter['word_nodes']}")
    print(f"  Total edges: {stats_no_filter['total_edges']}")
    
    print("\nğŸ”¹ CÃ“ lá»c tá»« loáº¡i (N,Np,V,A,Nc,M,R,P):")
    print(f"  Total nodes: {stats_with_filter['total_nodes']}")
    print(f"  Word nodes: {stats_with_filter['word_nodes']}")
    print(f"  Total edges: {stats_with_filter['total_edges']}")
    
    # Calculate reduction
    node_reduction = ((stats_no_filter['total_nodes'] - stats_with_filter['total_nodes']) / 
                     stats_no_filter['total_nodes'] * 100)
    word_reduction = ((stats_no_filter['word_nodes'] - stats_with_filter['word_nodes']) / 
                     stats_no_filter['word_nodes'] * 100)
    edge_reduction = ((stats_no_filter['total_edges'] - stats_with_filter['total_edges']) / 
                     stats_no_filter['total_edges'] * 100)
    
    print(f"\nâœ¨ HIá»†U QUáº¢ GIáº¢M NHIá»„U:")
    print(f"  Giáº£m {node_reduction:.1f}% tá»•ng sá»‘ nodes")
    print(f"  Giáº£m {word_reduction:.1f}% word nodes")
    print(f"  Giáº£m {edge_reduction:.1f}% tá»•ng sá»‘ edges")
    
    # Show filtered vs kept words
    print(f"\nğŸ” PHÃ‚N TÃCH CHI TIáº¾T:")
    show_filtered_words_analysis(context_sentences, claim_sentences)
    
    print(f"\nğŸ’¡ Káº¾T LUáº¬N:")
    print(f"  - POS filtering giÃºp giáº£m Ä‘Ã¡ng ká»ƒ sá»‘ lÆ°á»£ng nodes vÃ  edges")
    print(f"  - Loáº¡i bá» tá»« chá»©c nÄƒng, dáº¥u cÃ¢u, giá»¯ láº¡i tá»« cÃ³ Ã½ nghÄ©a")
    print(f"  - Cáº£i thiá»‡n hiá»‡u suáº¥t xá»­ lÃ½ vÃ  cháº¥t lÆ°á»£ng phÃ¢n tÃ­ch")

def show_filtered_words_analysis(context_sentences, claim_sentences):
    """PhÃ¢n tÃ­ch chi tiáº¿t cÃ¡c tá»« Ä‘Æ°á»£c lá»c vÃ  giá»¯ láº¡i"""
    
    important_pos_tags = {'N', 'Np', 'V', 'A', 'Nc', 'M', 'R', 'P'}
    
    kept_words = []
    filtered_words = []
    
    # Analyze all tokens
    all_tokens = []
    for sent_tokens in context_sentences.values():
        all_tokens.extend(sent_tokens)
    for sent_tokens in claim_sentences.values():
        all_tokens.extend(sent_tokens)
    
    for token in all_tokens:
        word = token["wordForm"]
        pos_tag = token.get("posTag", "")
        
        if pos_tag in important_pos_tags:
            kept_words.append((word, pos_tag))
        else:
            filtered_words.append((word, pos_tag))
    
    print(f"  Tá»« Ä‘Æ°á»£c GIá»® Láº I: {len(kept_words)} tá»«")
    kept_sample = kept_words[:10]
    for word, pos in kept_sample:
        print(f"    '{word}' ({pos})")
    if len(kept_words) > 10:
        print(f"    ... vÃ  {len(kept_words) - 10} tá»« khÃ¡c")
    
    print(f"  Tá»« bá»‹ Lá»ŒC Bá»: {len(filtered_words)} tá»«")
    filtered_sample = filtered_words[:10]
    for word, pos in filtered_sample:
        print(f"    '{word}' ({pos})")
    if len(filtered_words) > 10:
        print(f"    ... vÃ  {len(filtered_words) - 10} tá»« khÃ¡c")

if __name__ == "__main__":
    compare_graphs() 